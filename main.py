"""
ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤ë¥¼ í™œìš©í•œ ìˆœì°¨ì  ë²¤ì¹˜ë§ˆí¬ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import sys
import os
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ops import (
    NaiveGeGLU, OptimizedGeGLU,
    NaiveLinearCrossEntropy, OptimizedLinearCrossEntropy
)
from models import create_transformer
from benchmarks import BenchmarkVisualizer
from benchmarks.sequential_benchmark_runner import SequentialBenchmark


def setup_device():
    """GPU ì„¤ì • ë° í™•ì¸"""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is not available. This benchmark requires GPU.")
    
    device = "cuda:0"
    torch.cuda.set_device(device)
    
    # GPU ì •ë³´ ì¶œë ¥
    gpu_name = torch.cuda.get_device_name(device)
    gpu_memory = torch.cuda.get_device_properties(device).total_memory / (1024**3)
    
    print(f"Using GPU: {gpu_name}")
    print(f"GPU Memory: {gpu_memory:.1f} GB")
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"PyTorch Version: {torch.__version__}")
    
    return device, gpu_memory


def get_benchmark_config(gpu_memory: float, mode: str = "quick"):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì„¤ì •"""
    if gpu_memory < 4:
        config = {
            'vocab_size': 8000,
            'd_model': 128,
            'n_heads': 4,
            'n_layers': 2,
            'd_ff': 512,
            'batch_size': 2,
            'seq_len': 128,
            'max_linear_size': 512
        }
    elif gpu_memory < 6:
        config = {
            'vocab_size': 16000,
            'd_model': 256,
            'n_heads': 4,
            'n_layers': 3,
            'd_ff': 1024,
            'batch_size': 4,
            'seq_len': 256,
            'max_linear_size': 1024
        }
    else:
        config = {
            'vocab_size': 32000,
            'd_model': 512,
            'n_heads': 8,
            'n_layers': 4,
            'd_ff': 2048,
            'batch_size': 8,
            'seq_len': 512,
            'max_linear_size': 2048
        }
    
    # comprehensive ëª¨ë“œì—ì„œëŠ” í¬ê¸° ì¦ê°€
    if mode == "comprehensive":
        config['batch_size'] = min(config['batch_size'] * 2, 16)
        config['seq_len'] = min(config['seq_len'] * 2, 1024)
    
    return config


def run_geglu_benchmark(benchmark: SequentialBenchmark, config: dict, device: str):
    """GeGLU ìˆœì°¨ì  ë²¤ì¹˜ë§ˆí¬"""
    print("\n" + "="*60)
    print("ğŸ”§ BENCHMARKING GEGLU OPERATION")
    print("="*60)
    
    hidden_dim = config['d_model']
    ff_dim = config['d_ff']
    data_size = config['batch_size'] * config['seq_len']
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = torch.randn(data_size, hidden_dim, device=device, requires_grad=True)
    
    def naive_creator():
        return NaiveGeGLU(hidden_dim, ff_dim)
    
    def optimized_creator():
        return OptimizedGeGLU(hidden_dim, ff_dim)
    
    benchmark.benchmark_operation_sequential(
        "GeGLU",
        naive_creator,
        optimized_creator,
        test_data,
        warmup_runs=5,
        profile_runs=20
    )


def run_linear_ce_benchmark(benchmark: SequentialBenchmark, config: dict, device: str):
    """Linear Cross Entropy ìˆœì°¨ì  ë²¤ì¹˜ë§ˆí¬"""
    print("\n" + "="*60)
    print("ğŸ”§ BENCHMARKING LINEAR CROSS ENTROPY")
    print("="*60)
    
    hidden_dim = config['d_model']
    vocab_size = config['vocab_size']
    data_size = min(config['max_linear_size'], config['batch_size'] * config['seq_len'])
    
    # ë©”ëª¨ë¦¬ ì¶”ì •
    estimated_memory = data_size * vocab_size * 4 / (1024**3)  # GB
    print(f"Estimated LinearCE memory: {estimated_memory:.2f} GB")
    
    # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸
    gpu_memory = torch.cuda.get_device_properties(device).total_memory / (1024**3)
    if estimated_memory > gpu_memory * 0.6:
        print("âš ï¸  Skipping LinearCE due to memory constraints")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    x = torch.randn(data_size, hidden_dim, device=device, requires_grad=True)
    targets = torch.randint(0, vocab_size, (data_size,), device=device)
    test_data = (x, targets)
    
    def naive_creator():
        return NaiveLinearCrossEntropy(hidden_dim, vocab_size, bias=False)
    
    def optimized_creator():
        return OptimizedLinearCrossEntropy(hidden_dim, vocab_size, bias=False)
    
    # ì»¤ìŠ¤í…€ ì‹¤í–‰ í•¨ìˆ˜ (lossë§Œ ë°˜í™˜í•˜ë„ë¡)
    class LinearCEWrapper:
        def __init__(self, model):
            self.model = model
        
        def __call__(self, test_data):
            x, targets = test_data
            loss, _ = self.model(x, targets)
            return loss
    
    def naive_creator_wrapped():
        model = NaiveLinearCrossEntropy(hidden_dim, vocab_size, bias=False)
        return LinearCEWrapper(model)
    
    def optimized_creator_wrapped():
        model = OptimizedLinearCrossEntropy(hidden_dim, vocab_size, bias=False)
        return LinearCEWrapper(model)
    
    benchmark.benchmark_operation_sequential(
        "LinearCE",
        naive_creator_wrapped,
        optimized_creator_wrapped,
        test_data,
        warmup_runs=3,
        profile_runs=10
    )


def run_model_benchmark(benchmark: SequentialBenchmark, config: dict, device: str):
    """ëª¨ë¸ ì „ì²´ ìˆœì°¨ì  ë²¤ì¹˜ë§ˆí¬"""
    print("\n" + "="*60)
    print("ğŸ—ï¸ BENCHMARKING TRANSFORMER MODEL")
    print("="*60)
    
    # ëª¨ë¸ ì„¤ì •
    model_config = {
        'vocab_size': config['vocab_size'],
        'd_model': config['d_model'],
        'n_heads': config['n_heads'],
        'n_layers': config['n_layers'],
        'd_ff': config['d_ff'],
        'max_seq_len': 1024,
        'dropout': 0.0
    }
    
    # ì…ë ¥ ë°ì´í„° (ëª¨ë¸ìš©ìœ¼ë¡œ ì‘ê²Œ)
    batch_size = min(config['batch_size'], 4)
    seq_len = min(config['seq_len'], 256)
    
    input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)
    targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)
    
    def naive_model_creator():
        return create_transformer(**model_config, use_optimized=False)
    
    def optimized_model_creator():
        return create_transformer(**model_config, use_optimized=True)
    
    benchmark.benchmark_model_sequential(
        "Transformer",
        naive_model_creator,
        optimized_model_creator,
        input_ids,
        targets,
        warmup_runs=3,
        profile_runs=10
    )


def create_visualizations(results: dict, output_dir: str):
    """ê²°ê³¼ ì‹œê°í™”"""
    memory_results = results.get('memory', {})
    speed_results = results.get('speed', {})
    
    if not memory_results and not speed_results:
        print("âŒ No data to visualize")
        return
    
    print("\n" + "="*60)
    print("ğŸ“Š CREATING VISUALIZATIONS")
    print("="*60)
    
    try:
        visualizer = BenchmarkVisualizer(output_dir)
        
        # ì—°ì‚°ë³„ ê²°ê³¼ (ëª¨ë¸ ê²°ê³¼ ì œì™¸)
        operation_memory = {k: v for k, v in memory_results.items() if not k.startswith('model_')}
        operation_speed = {k: v for k, v in speed_results.items() if not k.startswith('model_')}
        
        if operation_memory:
            print("ğŸ“Š Creating memory comparison...")
            visualizer.plot_memory_comparison(operation_memory, "sequential_memory_comparison")
        
        if operation_speed:
            print("âš¡ Creating speed comparison...")
            visualizer.plot_speed_comparison(operation_speed, "sequential_speed_comparison")
        
        # CSV ì €ì¥
        print("ğŸ’¾ Saving results to CSV...")
        visualizer.save_results_to_csv(memory_results, speed_results, "sequential_benchmark_results.csv")
        
        print(f"âœ… Results saved to: {output_dir}")
        
    except Exception as e:
        print(f"âŒ Error creating visualizations: {e}")


def print_summary(results: dict):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    memory_results = results.get('memory', {})
    speed_results = results.get('speed', {})
    
    print("\n" + "="*60)
    print("ğŸ‰ BENCHMARK SUMMARY")
    print("="*60)
    
    memory_improvements = []
    speed_improvements = []
    
    # ë©”ëª¨ë¦¬ ê°œì„ ìœ¨ ìˆ˜ì§‘
    for result in memory_results.values():
        if "improvement" in result:
            for imp in result["improvement"].values():
                if isinstance(imp, (int, float)):
                    memory_improvements.append(imp)
    
    # ì†ë„ ê°œì„ ìœ¨ ìˆ˜ì§‘
    for result in speed_results.values():
        if "speedup" in result:
            for speedup_info in result["speedup"].values():
                if isinstance(speedup_info, dict) and "improvement_pct" in speedup_info:
                    speed_improvements.append(speedup_info["improvement_pct"])
    
    if memory_improvements:
        print(f"ğŸ’¾ Memory Improvements:")
        print(f"  Average: {sum(memory_improvements) / len(memory_improvements):.1f}%")
        print(f"  Best:    {max(memory_improvements):.1f}%")
        print(f"  Worst:   {min(memory_improvements):.1f}%")
    
    if speed_improvements:
        print(f"\nâš¡ Speed Improvements:")
        print(f"  Average: {sum(speed_improvements) / len(speed_improvements):.1f}%")
        print(f"  Best:    {max(speed_improvements):.1f}%")
        print(f"  Worst:   {min(speed_improvements):.1f}%")
    
    total_benchmarks = len(memory_results) + len(speed_results)
    print(f"\nğŸ“Š Total benchmarks completed: {total_benchmarks}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Sequential Triton Optimization Benchmark")
    parser.add_argument("--mode", choices=["quick", "comprehensive"], default="quick",
                       help="Benchmark mode")
    parser.add_argument("--output-dir", default="benchmark_results",
                       help="Output directory")
    parser.add_argument("--operations", nargs="+", 
                       choices=["geglu", "linear_ce", "model"], 
                       default=["geglu", "linear_ce"],
                       help="Operations to benchmark")
    
    args = parser.parse_args()
    
    print("ğŸš€ ìˆœì°¨ì  Triton ìµœì í™” ë²¤ì¹˜ë§ˆí¬")
    print("="*60)
    
    try:
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device, gpu_memory = setup_device()
        
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = get_benchmark_config(gpu_memory, args.mode)
        
        print(f"ğŸ“‹ Mode: {args.mode}")
        print(f"ğŸ”§ GPU Memory: {gpu_memory:.1f} GB")
        print(f"ğŸ“¦ Batch size: {config['batch_size']}")
        print(f"ğŸ“ Sequence length: {config['seq_len']}")
        print(f"ğŸ“š Vocab size: {config['vocab_size']}")
        print(f"ğŸ§  Model dimension: {config['d_model']}")
        print(f"âš™ï¸  Operations: {args.operations}")
        
        # ìˆœì°¨ì  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°
        benchmark = SequentialBenchmark(device)
        
        # ì„ íƒëœ ì—°ì‚°ë“¤ ë²¤ì¹˜ë§ˆí¬
        if "geglu" in args.operations:
            try:
                run_geglu_benchmark(benchmark, config, device)
            except Exception as e:
                print(f"âŒ GeGLU benchmark failed: {e}")
        
        if "linear_ce" in args.operations:
            try:
                run_linear_ce_benchmark(benchmark, config, device)
            except Exception as e:
                print(f"âŒ LinearCE benchmark failed: {e}")
        
        if "model" in args.operations:
            try:
                run_model_benchmark(benchmark, config, device)
            except Exception as e:
                print(f"âŒ Model benchmark failed: {e}")
        
        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results = benchmark.get_results()
        
        if results['memory'] or results['speed']:
            # ì‹œê°í™” ìƒì„±
            create_visualizations(results, args.output_dir)
            
            # ìš”ì•½ ì¶œë ¥
            print_summary(results)
            
            print("\nğŸ‰ Benchmark completed successfully!")
        else:
            print("âŒ No successful benchmark runs")
            return 1
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())